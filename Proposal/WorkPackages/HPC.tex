\TOWRITE{ALL}{Proofread WP 5 High Performance Computing pass 2}



\begin{workpackage}[id=hpc,wphases=0-48,
  short=High Performance Math. Computing,% for Figure 5.
  title=High Performance Mathematical Computing,
  lead=UJF,
  USHRM=10, % Jupyter-SGE integration
  PSRM=6,   % HPC for Combinatorics 
  LLRM=12,  % Pythran
  SARM=18, % GAP
  UKRM=60, % Singular
  UBRM=36,  % Pari + HPC for Combinatorics + Sage integration
  UORM=2, % added 2PM for oxford after third amendment
  UJFRM=52, % Linbox / Pythran 
  LEEDSRM=6] %real PM is 6.26
  

\begin{wpobjectives}
  The objective of this work package is to improve the performance of
  the computational components of \TheProject, in particular on
  massively parallel architectures. This includes notably:
  \begin{compactitem}
  \item Fine grained High Performance Computing on many-cores architectures.
  \item Coarse grained or embarrassingly parallel computing on grids or on the cloud.
  \item Compilation of high level interpreted code to optimised parallel native code.
  \item Develop novel HPC infrastructure in the context of combinatorics.
  \end{compactitem}
  A key aspect will be to foster further sharing expertise and best
  practices between computational components.
\end{wpobjectives}

\begin{wpdescription}
  As in all other areas of science, properly supporting massively
  parallel architectures is a major challenge. Many of the
  computational components in \TheProject have already gone a long way
  in this direction. For example, parallel versions
  of the \GAP kernel for
  a range of architectures were developed during the 2009-2013 EPSRC
  ``HPCGAP'' project. The expertise
  gained there was then transferred to the ongoing \Singular-HPC
  project, in particular through the rehiring of one of the developers
  of HPC-GAP. The French ANR HPAC project (2012-2015) has also widely contributed to design
  parallel exact linear algebra kernels  which is a core component for most HPC
  applications. The \Linbox library, used in sage, has benefited from this
  experience on the multi-core processing aspects. 

  In this work package, we will build on this momentum to further implement HPC support in
  the component Tasks~\localtaskref{hpc-pari} (\Pari),\localtaskref{hpc-gap} (\GAP),
  \localtaskref{hpc-linbox} (\Linbox), \localtaskref{hpc-mpir}(\MPIR) and
  \localtaskref{hpc-singular} (\Singular).
  
  Many of the computational components of \TheProject, notably \Sage
  and \GAP use a high level
  interpreted language for their library. Performance is achieved by
  rewriting or compiling critical sections into a lower-level
  language. \Sage uses
  the \Cython \Python-to-C compiler; \GAP has some more basic support.
  In Tasks~\localtaskref{hpc-gap} and \localtaskref{pythran}, we will also boost performance by
  further developing and applying such compilation tools, allowing the
  application programmer to retain their high level approach.

\end{wpdescription}
\begin{tasklist}
\begin{task}[title=PARI,id=hpc-pari,PM=20, lead=UB,wphases=0-48!0.5,issue=99]
  \Pari is a C library mainly oriented toward arithmetic and number theory.
  
  It currently supports both POSIX threads or MPI but lacks interfaces for
  parallelism. More precisely, it should be easier from an external package
  or software (e.g. Sage) to better exploit \Pari parallel features.

  On the other hand, most basic algorithms in the PARI library (e.g. integer
  factorisation) are currently implemented using only one core. To
  make better use of multi-core architecture and more generally parallel
  architectures, we will devise a generic parallelisation machinery
  to allow individual implementations to scale gracefully between single
  core / multicore / massively parallel machines while avoiding code
  duplication.

  The deliverables for this task is \localdelivref{pari-hpc2}.
\end{task}


\begin{task}[title=GAP,id=hpc-gap,PM=18, lead=SA, wphases={0-48!0.375}, issue=100]
  Thanks to the HPCGAP project, almost the full functionality of \GAP
  can be safely run on multicore architectures, and there is support for
  simple but effective parallel programming, with protection from most
  of the more serious pitfalls that can trouble the novice parallel
  programmer. Experimental versions of \GAP also exist for a number of
  distributed-memory architectures.


In this task we will continue to develop the \GAP infrastructure to
offer performance improvements to real end users on a wide range of
modern hardware. This will be achieved by a number of synergistic
developments in the system

\begin{compactitem}
\item a library of parallel algorithms for algebraic computation. This
  will include general purpose skeletons applicable to many problems;
  distributed data structures suitable for orchestrating distributed
  memory computations; and implementations of specific parallel algorithms for key
  mathematical tasks.  Target skeletons include irregular parallel
  maps and folds; transitive closure operations, especially orbits of
  group actions and chain reduction (as used in Gaussian
  elimination). Data structure targets include distributed task
  queue, hash table and array structures.Specific algorithm targets include linear algebra over
  finite fields; randomised search algorithms in general and matrix group
  recognition in particular and algorithms for analysing the structure
  of groups given by a finite presentation
\item interfaces between \GAP and standard cloud and HPC
  infrastructure. This work will be based on \taskref{component-architecture}{component-for-HPC}.
  At the moment \GAP is designed for interactive use or use as a local
  (lacking resource control or security) SCSCP server. Data is taken
  from local files or fixed URLs. We will develop
  interfaces that make it easy to run \GAP jobs through standard batch
  queue environments; enable SCSCP servers to take advantage of
  widely available authentication and resource control frameworks in
  cloud and HPC settings, and access resources through standard
  discovery and allocation mechanisms.

\item adaptation of the cython and/or pythran technology to allow the
  performance of critical \GAP language subroutines to be increased
  close to that of C code without the programmer cost of a full C implementation.
\end{compactitem}
The deliverables for this task is  \delivref{hpc}{GAP-HPC-report}, a
report on all the \GAP-related activities of this workpackage,
including links to the software which will by then have been
publically released.

\end{task}


\begin{task}[title=Linbox,PM=40,id=hpc-linbox, lead=UJF,wphases={3-48!0.37,12-36},issue=101]
Most intensive mathematical computations rely heavily on exact linear kernels
and their ability to harness parallel computers, grids or clusters. The \Linbox
library, already delivers high sequential efficiency for mathematical software
such as \Sage. The parallelisation of the library for multi-core architectures
has been initiated in the A.N.R. HPAC project and successfully set the building
blocks for high performance algebraic computing. 
The task here is to  address the remaining challenges for the use of such
kernels through a general audience mathematical software, such as \Sage.
A first aspect focuses on code design and domain specific languages allowing to
expose an abstraction of the parallel infrastructure and the parallel features
of the code through the stack of libraries, and support the
composition of parallel routines. This will be addressed in
deliverable~\localdelivref{LinBox-distributed}. More generally the second aspect, addressed
in deliverable~\localdelivref{LinBox-algo} concerns the 
development of new parallel algorithms and implementations, that are still
barriers in the development of High performance mathematical
applications. Lastly, the third part, in
deliverable~\localdelivref{LinBox-distributed}, addresses the specificities of  distributed
computing, with a close focus on communications and heterogeneous infrastructures.

  % \begin{compactitem}
  % \item Large scale parallelization of core computations
  %   \begin{compactitem}
  %   \item Higher levels algorithms
  %   \item Shared memory parallelization
  %   \end{compactitem}
  % \item Closer integration of existing parallel features in \Sage
  %   \begin{compactitem}
  %   \item DSL, runtime
  %   \item resource management, etc
  %   \end{compactitem}
  % \end{compactitem}
  % \TOWRITE{JGD/CP}{Task around HPC/parallelism/perf in Linbox}

\end{task}

\begin{task}[title=Singular,lead=UK, PM=47, id=hpc-singular,wphases=0-48!0.9,issue=102]
% Researcher: 46PM -> 2-48
% Management (Wolfram): 1PM
%  \label{task:hpc-singular}
  The unique challenge of parallelizing Singular has been that it is a decades-old
  project, with a codebase exceeding 300,000 lines of code and an enormous existing
  investment of development effort. This makes a wholesale manual rewrite or reengineering
  approach infeasible.

  We therefore use a multi-pronged approach: First, we have created automated
  source-to-source translation tools that take existing C/C++ code as input and generate
  thread safe code as output. Secondly we are also adding facilities to the C/C++ code and
  the Singular interpreter to safely access shared memory. These facilities ensure in
  particular that common pitfalls of concurrent programming, such as data races and
  deadlocks, cannot occur. For this, we leverage approaches that have already been
  successfully used for HPC-GAP and whose principles are well-understood.

  To supplement the above existing work, we propose to add very fine-grained parallelism
  to some key components of Singular. These include writing a multi-threaded
  implementation of the Singular multivariate polynomial arithmetic, of the main quadratic
  sieve implementation for integer factoring and parallelisation of the FFT based integer
  and dense polynomial multiplication algorithm. These key components are used extensively
  for Singular's overall workload, including in the Groebner basis engine and polynomial
  subsystems. Performance increases through fine-grained parallelisation of key components
  such as these will provide extensive benefits to virtually all users of Singular on
  multi-core machines.
  Output are \localdelivref{QS-linalg}, \localdelivref{FFT} and~\localdelivref{singular-polyarith}.
\end{task}

\begin{task}[title=\MPIR,id=hpc-mpir, lead=UK,PM=13,wphases=6-18,issue=103]
% Researcher: 12PM
% Wolfram (Management): 1PM
\MPIR (Multiple Precision Integers and Rationals) is the core library in \Sage
for bignum arithmetic. It is used extensively by a majority of the core C/C++
libraries in \Sage, and by \Sage directly via \Cython. \MPIR is a fork of the 
GMP (Gnu Multi-precision) library, with many independent implementations of the
core algorithms (including a faster FFT and division code, better 
superoptimisation on some common 64 bit processors and native MSVC support). 
It consists of around 250,000 lines of code, much of which is assembly 
primitives and very low level, highly optimised C code.

Maintenance of \MPIR is not merely a matter of updating the build system.
Rather, every time a new processor is released by AMD, Intel, Sparc or ARM,
significant development has to be invested in hand-optimising and then
superoptimising assembly code for the new processors. This gives up to a 12x
performance increase over optimised C code, due to the specialised nature of
bignum arithmetic, which is in some sense a worst case for C compilers. Indeed
without continuous effort, \MPIR would not even run on new operating systems and
processors, let alone run fast. This is a unique problem that assembly libraries
have.

As a successful and key component of \Sage, we believe it is time to invest in
maintenance and improvement of \MPIR by hiring an assembly expert who can work
full time on the project after \MPIR's lead assembly expert sadly passed
away recently. Significant challenges exist, such as
optimising for SIMD instruction sets. Without investment into maintenance,
assembly superoptimisation, processor support, fat binary support, etc. this key
component of \Sage will fall behind, to the detriment of \Sage as a whole and the
numerous other standalone libraries that make use of \MPIR.

Output is \localdelivref{MPIRsuperoptimiser} and a contribution to \localdelivref{FFT}.
\end{task}

\begin{task}[title=HPC infrastructure for combinatorics,id=hpc-combi,PM=26,lead=PS,partners={UB},wphases={0-6!0.3,12-36!0.5},issue=104]
  Several members of the projects are experts in combinatorics a field where
  Sage is clearly a world leader~\cite{Sage-Combinat}. This particular research
  field has several specific features which makes it interesting from the HPC
  point of view.

  The most important feature is that the goal of research is mostly to
  design and to understand properties of algorithms. As a consequence,
  much more often that in other field, the researcher needs to
  program. However, this is not his ultimate goal so the programming
  environment must be very expressive for fast prototyping.

  At the same time, the problems often require relatively large
  computations; algorithms of exponential complexity are extremely
  common, and combinatorial explosion is the main obstacle to many
  experiments. Hence the programming environment must make no
  compromise on efficiency.

  Finally, embarrassingly parallel problem are extremely common, and
  more often than not problems that are not embarrassingly parallel
  reduce to the exploration of a large tree. Hence the programming
  environment must minimise the extra work needed to get from a serial
  program to a parallel one in these simple situations.

  Through this task we will provide a concrete, practical, and highly
  demanding use cases for the infrastructure developed in this work
  package. In particular, they will serve to evaluate the benefits of
  tasks~\localtaskref{pythran},
  \taskref{component-architecture}{mod-packaging}, and
  \taskref{component-architecture}{component-for-HPC}.
  In particular, we will provide a mixed C/Python implementation that
  will be integrated within Sage and replace most of the Sage-combinat
  code~(deliverable~\localdelivref{sage-paral-tree} 
  and \localdelivref{sage-HPCcombi}).

  In a second and more exploratory direction, some
  experiments~\cite{FromentinHivert} shows that the large tree exploration
  problem is very easily solved in C++ using the new Intel
  \texttt{Cilk++}~\cite{CilkIntel,CilkRefman} technology (See for example:\\
  \href{https://github.com/jfromentin/nsgtree}{https://github.com/jfromentin/nsgtree}
  and
  \href{https://github.com/hivert/IVMPG}{https://github.com/hivert/IVMPG}). We
  would like to explore the possibility to interface smoothly \Pythran,
  \Cython, and \texttt{Cilk++} (deliverable~\localdelivref{cython-pythran-cilk}).
\end{task}

\begin{task}[title=Pythran,id=pythran,lead=LL,partners={UJF,UO},PM=26, wphases=0-24, issue=105]
  \Cython (a fork of \texttt{Pyrex}) is an extended-\Python to C
  compiler that has received significant contributions from \Sage
  developers, and is a thriving project of its own.

  \Pythran and \Cython are similar in spirit but have complementary feature
  sets: \Pythran can heavily optimise high level \Numpy constructs and \Cython
  has broader \Python support. In this task, we will investigate the
  opportunity and feasibility of a convergence between \Cython and \Pythran.
  \begin{compactitem}
    \item depending on the code at hand, one strategy or the other would be automatically selected.
    \item The optimised runtime of \Pythran can be used through \Cython.
  \end{compactitem}
  An effort will be made to improve more and more the parallelism in the
  \Pythran runtime.

  This work will be achieved through a close collaboration between the \Pythran
  developers hired for \TheProject and \Cython developers involved in the \Sage
  project. It should quicken \Sage execution time at least on \Numpy centric
  codes, while not putting an extra burden on the developers.  Preliminary
  discussions with the \Cython community have already taken place and received a
  very favorable feedback.

  Adding \Pythran support in \Sage will be done not only for \Sage code but also
  for \Sage users code thanks to compilation facilities in the notebook interface.
  Output is \localdelivref{pythran-sage}.

%  In a similar perspective, testing and improving the integration between
%  \software{mpi4py} and \Pythran could provide an efficient toolchain for HPC
%  while keeping full backward compatibility with pure \Python code. This will
%  required a continuous integration of \Pythran to ensure its capabilities
%  \localdelivref{pythran}.

  Internally, \Sage uses \Cython for compiling the critical sections of
  its libraries. In this task, we will explore opportunities to
  benefit from \Pythran compilation within the \Sage library to benefit
  \Pythran compile time optimisation. A specific challenge is that the \Sage
  library uses quite heavily object-oriented programming.

  A first step to support object-oriented programming will be to make
  \Pythran type inference more accurate, which will also improve error
  feedback provided for the user. Output is \localdelivref{pythran-typing}.

  An exploratory part of the task will be to study CERN's \href{https://root.cern.ch/}{Root}
  software system and see how it can streamline the integration of C/C++ software in \Sage.
  This will require work on support for \clang and \cling in
  \taskref{component-architecture}{portability}.
\end{task}

%Stuart Mumford, Neil Lawrence and Mike Croucher, Sheffield
\begin{task}[title=Sun Grid Engine Integration in Project \Jupyter Hub, lead=USH,id=hpc-jupyter,PM=12,wphases=0-12,issue=106]
The Sun Grid Engine is a commonly used High Performance Computing
scheduler. It is used, for example, on the institutional HPC systems
in both Sheffield and Manchester Universities as well as the regional
N8 HPC facility, a system shared by the 8 most research intensive
universities in the North of England. In this task, we will develop
and demonstrate a Sun Grid Engine notebook spawner for Project
\Jupyter, allowing users to access \Jupyter notebooks on the HPC
cluster. This will enable the interactive analysis of output data
products on the cluster where they were generated and are stored, via
a user-friendly web interface \localdelivref{SGE-jupyter}.
\end{task}
\end{tasklist}

\begin{wpdelivs}
  \begin{wpdeliv}[due=3,miles=startup,id=sage-paral-tree,dissem=PU,nature=DEM,lead=PS,issue=107, status=delivered]
      {Turn the Python prototypes for tree exploration into production code, integrate to \Sage.}
\end{wpdeliv}
  \begin{wpdeliv}[due=18,miles=startup,id=pythran-sage,dissem=PU,nature=DEM,lead=UJF,issue=115, status=delivered]
      {Facility to compile \Pythran compliant user kernels and \Sage code and automatically
       take advantage of multi-cores and SIMD instruction units in \Cython}
  \end{wpdeliv}
%  \begin{wpdeliv}[due=6,id=QS-sieving,dissem=PU,nature=DEM,lead=UK]
 %     {Parallelise the relation sieving component of the Quadratic Sieve}
%  \end{wpdeliv}
  \begin{wpdeliv}[due=12,miles=startup,id=SGE-jupyter,dissem=PU,nature=OTHER,lead=USH,issue=116, status=delivered]
      {Sun Grid Engine support for Project \Jupyter Hub}
  \end{wpdeliv}
  \begin{wpdeliv}[due=18,miles=startup,id=pythran-typing,dissem=PU,nature=DEM, lead=LL,issue=117, status=delivered]
      {Make \Pythran typing better to improve error information.}
  \end{wpdeliv}
  \begin{wpdeliv}[due=18,miles=proto1,id=MPIRsuperoptimiser,dissem=PU,nature=DEM,lead=UK,issue=118, status=delivered, blog=https://wbhart.blogspot.fr/2017/02/assembly-superoptimisation-in-mpir.html]
      {Write an assembly superoptimiser supporting AVX and upcoming Intel processor extensions for the \MPIR library and optimise MPIR for modern processors.}
\end{wpdeliv}
  \begin{wpdeliv}[due=18,miles=proto1,id=QS-linalg,dissem=PU,nature=DEM,lead=UK,issue=119,
    status=delivered, blog={https://wbhart.blogspot.fr/2017/02/integer-factorisation-in-flint.html}]
      {Parallelise the relation sieving component of the Quadratic Sieve and implement a parallel version of Block-Wiederman linear algebra over GF2 and implement large prime variants}
  \end{wpdeliv}
  \begin{wpdeliv}[due=18,miles=proto1,id=FFT,dissem=PU,nature=DEM, lead=UK,issue=120, status=delivered, blog=https://wbhart.blogspot.fr/2017/02/parallelising-integer-and-polynomial.html]
    {Take advantage of multiple cores in the matrix Fourier Algorithm component of the FFT for integer and polynomial arithmetic,and include assembly primitives for SIMD processor instructions (e.g. AVX, etc.), especially in the FFT butterflies.}
\end{wpdeliv}
  %% \begin{wpdeliv}[due=18,miles=proto1,id=GAP-hpc-report,dissem=PU,nature=R,lead=SA]
  %%   {Report on development of designs for the \GAP developments --
  %%     parallel library, interacts to standard infrastructure and
  %%     \Cython-like extensions }
  %% \end{wpdeliv}
  \begin{wpdeliv}[due=24,miles=proto1,id=cython-pythran-cilk,dissem=PU,nature=DEM,lead=PS,issue=121,status=canceled]
    {Explore the possibility to interface smoothly \Pythran, \Cython and \texttt{Cilk++}.}
    This deliverable was merged into~\localdelivref{sage-HPCcombi}.
  \end{wpdeliv}
  \begin{wpdeliv}[due=24,miles=proto1,id=LinBox-DSL,dissem=PU,nature=R,lead=UJF,issue=122,status=canceled]
    {Library design and domain specific language exposing \Linbox parallel features to \Sage}
    This deliverable was merged into~\localdelivref{LinBox-distributed}.
  \end{wpdeliv}
  \begin{wpdeliv}[due=24,id=pari-hpc1,dissem=PU,nature=DEM,lead=UB,issue=108,status=canceled]
    {Devise a generic parallelisation engine for \Pari and use it to prototype selected functions
      (integer factorisation, discrete logarithm, modular polynomials)}
    This deliverable was merged into~\localdelivref{pari-hpc2}.
  \end{wpdeliv}
  \begin{wpdeliv}[due=36,miles=hpc-prototype,id=sage-HPCcombi,dissem=PU,nature=DEM,lead=UB,issue=109,status=delivered]
      {Refactor and Optimise the existing combinatorics \Sage code using the new developed \Pythran and \Cython features.}
  \end{wpdeliv}
%   \begin{wpdeliv}[due=24,id=MPIRfat,dissem=PU,nature=DEM,lead=UK]
%       {Provide FAT binary support for all new x86\_64 processors, build system
%         maintenance and improvements to tuning, profiling and testing subsystems
%       for the \MPIR library.}
% \end{wpdeliv}
%  \begin{wpdeliv}[due=36,id=singular-polymul,dissem=PU,nature=DEM,lead=UK]
%      {Parallelise the Singular sparse polynomial multiplication algorithms}
%\end{wpdeliv}
  \begin{wpdeliv}[due=36,miles=hpc-prototype,id=LinBox-algo,dissem=PU,nature=DEM, lead=UJF,issue=110,status=delivered]
    {Exact linear algebra algorithms and implementations. Library maintenance and close integration
      in mathematical software for \Linbox library}
  \end{wpdeliv}
  % \begin{wpdeliv}[due=48,id=MPIRprocessors,dissem=PU,nature=DEM,lead=UK]
  %     {Ongoing support of Intel, AMD, ARM, Sparc processors and new Operating
  %       System versions for the \MPIR library.}
  % \end{wpdeliv}
  % \begin{wpdeliv}[due=36,id=pari-hpc2,dissem=PU,nature=R,lead=UB]
  % {Identify the list of all the functions that could benefit from the generic parallelization engine}
  % \end{wpdeliv}
  %% \begin{wpdeliv}[due=47,miles=eval,id=GAP-software-final,dissem=PU,nature=OTHER,lead=SA]
  %%     {Implementations of the \GAP developments, ready for release}
  %% \end{wpdeliv}
  \begin{wpdeliv}[due=48,miles=eval,id=singular-polyarith,dissem=PU,nature=DEM, lead=UK,issue=111,status=submitted]
      {Parallelise the Singular sparse polynomial multiplication algorithms and
        provide parallel versions of the Singular sparse polynomial division and GCD algorithms.}
\end{wpdeliv}
  \begin{wpdeliv}[due=48,miles=eval,id=LinBox-distributed,dissem=PU,nature=DEM, lead=UJF,issue=112,status=submitted]
    {Implementations of exact linear algebra algorithms on distributed memory et heterogenous
      architectures: clusters and accelerators. Solving large linear systems
      over the rationals is the target application.} 
  \end{wpdeliv}
  \begin{wpdeliv}[due=48,miles=eval,id=GAP-HPC-report,dissem=PU,nature=R,lead=SA,issue=113,status=submitted]
      {Final report and evaluation of all the \GAP developments.}
  \end{wpdeliv}
  \begin{wpdeliv}[due=48,id=pari-hpc2,dissem=PU,nature=DEM,lead=UB,issue=114,status=submitted]
  {\Pari suite release (\libpari, \GP and \GPtoC) that fully support parallelisation
   allowing individual implementations to scale gracefully between single
   core / multicore / massively parallel machines.}
  \end{wpdeliv}

\end{wpdelivs}
\end{workpackage}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../proposal"
%%% End:


%  LocalWords:  workpackage hpc wphases Pythran Linbox wpobjectives wpdescription texttt
%  LocalWords:  localtaskref Cython tasklist TOWRITE parallelisation Groebner bignum Cilk
%  LocalWords:  superoptimisation optimised Sparc hand-optimising superoptimising Numpy
%  LocalWords:  specialised optimising hpc-combi deployment-distrib delivref mpi4py
%  LocalWords:  optimisations wpdelivs wpdeliv dissem LinBox-algo Parallelise QS-linalg
%  LocalWords:  Block-Wiederman singular-polymul singular-polyarith MPIRsuperoptimiser
%  LocalWords:  superoptimiser MPIRprocessors MPIRfat HPCcombi compactitem taskref
%  LocalWords:  localdelivref optimisation Jupyter hpc-mpir Sage-Combinat FromentinHivert
%  LocalWords:  CilkRefman sage-paral-tree randomised analysing
