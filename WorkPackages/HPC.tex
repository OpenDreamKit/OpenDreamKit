\begin{draft}
\TOWRITE{UJF (Work Package Lead)}{For WP leaders, please check the following (remove items
once completed)}
\begin{verbatim}
- [X] have all deliverables in the WP a lead institution?
- [X] do all tasks list all sites involved in them? 
- [ ] does the table of sites and their PM efforts match lists of sites for each task?
      (each site from the table is listed in all relevant tasks, and no site is listed
      only in the table or only at some task)
\end{verbatim}
\end{draft}



\begin{workpackage}[id=hpc,wphases=0-48,
  title=High Performance Mathematical Computing,
  lead=UJF,
  USHRM=6, % Jupyter-SGE integration
  PSRM=6,   % HPC for Combinatorics 
  LLRM=12,  % Pythran
  SARM=18, % GAP
  UKRM=96, % Singular
  UBRM=40,  % Pari + HPC for Combinatorics + Sage integration
  UJFRM=52] % Linbox / Pythran
  

\begin{wpobjectives}
  The objective of this work package is to improve the performance of
  the computational components of \TheProject, in particular on
  massively parallel architectures. This includes notably:
  \begin{compactitem}
  \item Fine grained High Performance Computing on many-cores architectures.
  \item Coarse grained or embarrassingly parallel computing on grids or on the cloud.
  \item Compilation of high level interpreted code to optimized parallel native code.
  \item Develop novel HPC infrastructure in the context of combinatorics.
  \end{compactitem}
  A key aspect will be to foster further sharing expertise and best
  practices between computational components.
\end{wpobjectives}

\begin{wpdescription}
  As in all other areas of science, properly supporting massively
  parallel architectures is a major challenge. Many of the
  computational components in \TheProject have already gone a long way
  in this direction. For example, parallel versions
  of the \GAP kernel for
  a range of architectures were developed during the 2009-2013 EPSRC
  ``HPCGAP'' project. The expertise
  gained there was then transferred to the ongoing \Singular-HPC
  project, in particular through the rehiring of one of the developers
  of HPC-GAP. The French ANR HPAC project (2012-2015) has also widely contributed to design
  parallel exact linear algebra kernels  which is a core component for most HPC
  applications. The \Linbox library, used in sage, has benefited from this
  experience on the multi-core processing aspects. 

  In this work package, we will build on this momentum to further implement HPC support in
  the component Tasks~\localtaskref{hpc-pari},\localtaskref{hpc-gap},
  \localtaskref{hpc-linbox}, \localtaskref{hpc-mpir} and \localtaskref{hpc-singular}.
  
  Many of the computational components of \TheProject, notably \Sage
  and \GAP use a high level
  interpreted language for their library. Performance is achieved by
  rewriting or compiling critical sections into a lower-level
  language. \Sage uses
  the \Cython \Python-to-C compiler; \GAP has some more basic support.
  In Tasks~\localtaskref{hpc-gap}, \localtaskref{pythran-cython}
  and~\localtaskref{pythran-sage}, we will also boost performance by
  further developing and applying such compilation tools, allowing the
  application programmer to retain their high level approach.

\end{wpdescription}
\begin{tasklist}
\begin{task}[title=PARI,id=hpc-pari,PM=20, lead=UB, partners=UB]
  \Pari is a C library mainly oriented toward arithmetic and number theory.
  
  It currently supports both POSIX threads or MPI but lacks interfaces for
  parallelism. More precisely, it should be easier from an external package
  or software (e.g. Sage) to better exploit \Pari parallel features.

  On the other hand, most basic algorithms in the PARI library (e.g. integer
  factorization) are currently implemented using only one core. To
  make better use of multi-core architecture and more generally parallel
  architectures, we will devise a generic parallelization machinery
  to allow individual implementations to scale gracefully between single
  core / multicore / massively parallel machines while avoiding code
  duplication.

  The deliverables for this task are \delivref{hpc}{QS-sieving} and \delivref{hpc}{QS-linalg}.
\end{task}


\begin{task}[title=GAP,id=hpc-gap,PM=18, lead=SA, partners=SA]
  Thanks to the HPCGAP project, almost the full functionality of \GAP
  can be safely run on multicore architectures, and there is support for
  simple but effective parallel programming, with protection from most
  of the more serious pitfalls that can trouble the novice parallel
  programmer. Experimental versions of \GAP also exist for a number of
  distributed-memory architectures.


In this task we will continue to develop the \GAP infrastructure to
offer performance improvements to real end users on a wide range of
modern hardware. This will be achieved by a number of synergistic
developments in the system

\begin{itemize}
\item a library of parallel algorithms for algebraic computation. This
  will include general purpose skeletons applicable to many problems;
  distributed data structures suitable for orchestrating distributed
  memory computations; and implementations of specific parallel algorithms for key
  mathematical tasks.  
  \TOWRITE{SL/AK}{maybe list some parallel algorithms for algebraic computation}
\item interfaces between \GAP and standard cloud and HPC
  infrastructure. This work will be based on \taskref{component-architecture}{component-for-HPC}.
\TOWRITE{SL/AK}{Say more, batch schedulers, authentication for
    SCSCP servers, storage management, checkpointing}
\item adaptation of the cython and/or pythran technology to allow the
  performance of critical \GAP language subroutines to be increased
  close to that of C code without the programmer cost of a full C implementation.
\end{itemize}
\end{task}

\begin{task}[title=Linbox,PM=40,id=hpc-linbox, lead=UJF, partners=UJF]
Most intensive mathematical computations rely heavily on exact linear kernels
and their ability to harness parallel computers, grids or clusters. The \Linbox
library, already delivers high sequential efficiency for mathematical software
such as \Sage. The parallelization of the library for multi-core architectures
has been initiated in the A.N.R. HPAC project and successfully set the building
blocks for high performance algebraic computing. 
The task here is to  address the remaining challenges for the use of such
kernels through a general audience mathematical software, such as \Sage.
A first aspect focuses on code design and domain specific languages allowing to
expose an abstraction of the parallel infrastructure and the parallel features
of the code through the stack of libraries, and support the
composition of parallel routines.  More generally the second aspect concerns the
development of new parallel algorithms and implementations, that are still
barriers in the development of High performance mathematical
applications. Lastly, the third part addresses the specificities of  distributed
computing, with a close focus on communications and heterogeneous infrastructures.

  % \begin{compactitem}
  % \item Large scale parallelization of core computations
  %   \begin{compactitem}
  %   \item Higher levels algorithms
  %   \item Shared memory parallelization
  %   \end{compactitem}
  % \item Closer integration of existing parallel features in \Sage
  %   \begin{compactitem}
  %   \item DSL, runtime
  %   \item resource management, etc
  %   \end{compactitem}
  % \end{compactitem}
  % \TOWRITE{JGD/CP}{Task around HPC/parallelism/perf in Linbox}

\end{task}

\begin{task}[title=Singular,lead=UK, id=hpc-singular,partners=UK ]
  \label{task:hpc-singular}
  The unique challenge of parallelizing Singular has been that it is a decades-old
  project, with a codebase exceeding 300,000 lines of code and an enormous existing
  investment of development effort. This makes a wholesale manual rewrite or reengineering
  approach infeasible.

  We therefore use a multi-pronged approach: First, we have created automated
  source-to-source translation tools that take existing C/C++ code as input and generate
  thread safe code as output. Secondly we are also adding facilities to the C/C++ code and
  the Singular interpreter to safely access shared memory. These facilities ensure in
  particular that common pitfalls of concurrent programming, such as data races and
  deadlocks, cannot occur. For this, we leverage approaches that have already been
  successfully used for HPC-GAP and whose principles are well-understood.

  To supplement the above existing work, we propose to add very fine-grained parallelism
  to some key components of Singular. These include writing a multi-threaded
  implementation of the Singular multivariate polynomial arithmetic, of the main quadratic
  sieve implementation for integer factoring and parallelisation of the FFT based integer
  and dense polynomial multiplication algorithm. These key components are used extensively
  for Singular's overall workload, including in the Groebner basis engine and polynomial
  subsystems. Performance increases through fine-grained parallelisation of key components
  such as these will provide extensive benefits to virtually all users of Singular on
  multi-core machines.
\end{task}

\begin{task}[title=\MPIR,id=hpc-mpir, lead=UK,PM=12, partners=UK]
\MPIR (Multiple Precision Integers and Rationals) is the core library in \Sage
for bignum arithmetic. It is used extensively by a majority of the core C/C++
libraries in \Sage, and by \Sage directly via \Cython. \MPIR is a fork of the 
GMP (Gnu Multi-precision) library, with many independent implementations of the
core algorithms (including a faster FFT and division code, better 
superoptimisation on some common 64 bit processors and native MSVC support). 
It consists of around 250,000 lines of code, much of which is assembly 
primitives and very low level, highly optimised C code.

Maintenance of \MPIR is not merely a matter of updating the build system.
Rather, every time a new processor is released by AMD, Intel, Sparc or ARM,
significant development has to be invested in hand-optimising and then
superoptimising assembly code for the new processors. This gives up to a 12x
performance increase over optimised C code, due to the specialised nature of
bignum arithmetic, which is in some sense a worst case for C compilers. Indeed
without continuous effort, \MPIR would not even run on new operating systems and
processors, let alone run fast. This is a unique problem that assembly libraries
have.

As a successful and key component of \Sage, we believe it is time to invest in
maintenance and improvement of \MPIR by hiring an assembly expert who can work
full time on the project after \MPIR's lead assembly expert sadly passed
away recently. Significant challenges exist, such as
optimising for SIMD instruction sets. Without investment into maintenance,
assembly superoptimisation, processor support, fat binary support, etc. this key
component of \Sage will fall behind, to the detriment of \Sage as a whole and the
numerous other standalone libraries that make use of \MPIR.
\end{task}

\begin{task}[title=HPC infrastructure for combinatorics,id=hpc-combi,PM=26,lead=PS,partners={PS,UB}]

  Several members of the projects are experts in combinatorics a field where
  Sage is clearly a world leader~\cite{Sage-Combinat}. This particular research
  field has several specific features which makes it interesting from the HPC
  point of view.

  The most important feature is that the goal of research is mostly to
  design and to understand properties of algorithms. As a consequence,
  much more often that in other field, the researcher needs to
  program. However, this is not his ultimate goal so the programming
  environment must be very expressive for fast prototyping.

  At the same time, the problems often require relatively large
  computations; algorithms of exponential complexity are extremely
  common, and combinatorial explosion is the main obstacle to many
  experiments. Hence the programming environment must make no
  compromise on efficiency.

  Finally, embarrassingly parallel problem are extremely common, and
  more often than not problems that are not embarrassingly parallel
  reduce to the exploration of a large tree. Hence the programming
  environment must minimize the extra work needed to get from a serial
  program to a parallel one in these simple situations.

  Through this task we will provide a concrete, practical, and highly
  demanding use cases for the infrastructure developed in this work
  package. In particular, they will serve to evaluate the benefits of
  tasks~\localtaskref{cython},\localtaskref{pythran},
  \taskref{component-architecture}{mod-packaging}, and
  \taskref{component-architecture}{component-for-HPC}.
  In particular, we will provide a mixed C/Python implementation that
  will be integrated within Sage and replace most of the Sage-combinat code.

  In a second and more exploratory direction, some
  experiments~\cite{FromentinHivert} shows that the large tree exploration
  problem is very easily solved in C++ using the new Intel
  \texttt{Cilk++}~\cite{CilkIntel,CilkRefman} technology (See for example:
  \href{https://github.com/jfromentin/nsgtree}{https://github.com/jfromentin/nsgtree}
  and
  \href{https://github.com/hivert/IVMPG}{https://github.com/hivert/IVMPG}). We
  would like to explore the possibility to interface smoothly \Pythran,
  \Cython, and \texttt{Cilk++}.
\end{task}

\begin{task}[title=Pythran,id=pythran,lead=UJF,partners={UJF,LL},PM=24]
  \Cython is a extended-\Python to C compiler that was originally developed for
  \Sage and is now a thriving project of its own.

  \Pythran and \Cython are similar in spirit but have complementary feature
  sets: \Pythran can heavily optimize high level \Numpy constructs and \Cython
  has broader \Python support. In this task, we will investigate the
  opportunity and feasibility of a convergence between \Cython and \Pythran.
  \begin{itemize}
    \item depending on the code at hand, one strategy or the other would be automatically selected.
    \item The optimized runtime of \Pythran can be used through \Cython.
  \end{itemize}
  An effort will be made to improve more and more the parallelism in the
  \Pythran runtime \localdelivref{pythran-cython}.

  This work will be achieved through a close collaboration between the \Pythran
  developers hired for \TheProject and \Cython developers involved in the \Sage
  project. It should quicken \Sage execution time at least on \Numpy centric
  codes, while not putting an extra burden on the developers.  Preliminary
  discussions with the \Cython community have already taken place and received a
  very favorable feedback.

  Adding \Pythran support in \Sage will be done not only for \Sage code but also
  for \Sage users code thanks to compilation facilities in the notebook interface.
  \localdelivref{pythran-sage}

%  In a similar perspective, testing and improving the integration between
%  \software{mpi4py} and \Pythran could provide an efficient toolchain for HPC
%  while keeping full backward compatibility with pure \Python code. This will
%  required a continuous integration of \Pythran to ensure its capabilities
%  \localdelivref{pythran}.

  Internally, \Sage uses \Cython for compiling the critical sections of
  its libraries. In this task, we will explore opportunities to
  benefit from \Pythran compilation within the \Sage library to benefit
  \Pythran compile time optimisation. A specific challenge is that the \Sage
  library uses quite heavily object-oriented programming.

  This task will strongly benefit from Task~\localtaskref{pythran-cython},
  while providing in return a real life large-scale use case for it.

  A first step to support object-oriented programming will be to make
  \Pythran type inference more accurate, which will also improve error
  feedback provided for the user \localdelivref{pythran-typing}.
\end{task}

%Stuart Mumford, Neil Lawrence and Mike Croucher, Sheffield
\begin{task}[title=Sun Grid Engine Integration in Project Jupyter Hub, lead=USH,id=hpc-jupyter,PM=6,partners=USH]
The Sun Grid Engine is a commonly used High Performance Computing scheduler. It is used, for example, on the institutional HPC systems in both Sheffield and Manchester Universities as well as the regional N8 HPC facility, a system shared by the 8 most research intensive universities in the North of England. In this task, we will develop and demonstrate a Sun Grid Engine notebook spawner for Project Jupyter, allowing users to access Jupyter notebooks on the HPC cluster. This will enable the interactive analysis of output data products on the cluster where they were generated and are stored, via a user-friendly web interface \localdelivref{SGE-jupyter}.
\end{task}

\end{tasklist}
\begin{wpdelivs}
  \begin{wpdeliv}[due=3,id=pythran-sage,dissem=PU,nature=DEM,lead=UJF]
      {Facility to compile \Pythran compliant user kernels and sage code.}
  \end{wpdeliv}
  \begin{wpdeliv}[due=6,id=QS-sieving,dissem=PU,nature=DEM,lead=UB]
      {Parallelise the relation sieving component of the Quadratic Sieve}
  \end{wpdeliv}
  \begin{wpdeliv}[due=3,id=HPCcombi,dissem=PU,nature=DEM,lead=PS]
      {Turn the Python prototypes for tree exploration into production code, integrate to \Sage.}
\end{wpdeliv}
  \begin{wpdeliv}[due=9,id=pythran-cython,dissem=PU,nature=DEM,lead=LL]
      {Improve \Pythran runtime support to automatically take advantage of multi-cores and SIMD instruction units and use it in \Cython.}
  \end{wpdeliv}
  \begin{wpdeliv}[due=18,id=QS-linalg,dissem=PU,nature=DEM,lead=UB]
      {Implement a parallel version of Block-Wiederman linear algebra over GF2 and the triple large prime variant.}
  \end{wpdeliv}
  \begin{wpdeliv}[due=12,id=SGE-jupyter,dissem=PU,nature=OTHER,lead=USH]
      {Add Sun Grid Engine support to Project Jupyter Hub}
  \end{wpdeliv}
  \begin{wpdeliv}[due=12,id=pythran-typing,dissem=PU,nature=DEM, lead=UJF]
      {Make \Pythran typing better to improve error information.}
  \end{wpdeliv}
  \begin{wpdeliv}[due=12,id=FFT,dissem=PU,nature=DEM, lead=UK]
    {Take advantage of multiple cores in the matrix Fourier Algorithm component of the FFT
      for integer and polynomial arithmetic,and include assembly primitives for SIMD
      processor instructions (AVX, Knight's Bridge, etc.), especially in the FFT
      butterflies}
\end{wpdeliv}
  \begin{wpdeliv}[due=12,id=singular-polymul,dissem=PU,nature=DEM,lead=UK]
      {Parallelise the Singular sparse polynomial multiplication algorithms}
\end{wpdeliv}
  \begin{wpdeliv}[due=12,id=singular-polyarith,dissem=PU,nature=DEM, lead=UK]
      {Parallel versions of the Singular sparse polynomial division and GCD algorithms.}
\end{wpdeliv}
  \begin{wpdeliv}[due=12,id=MPIRsuperoptimiser,dissem=PU,nature=DEM,lead=UK]
      {Extend the existing assembly superoptimiser for AVX and upcoming Intel
        processor extensions for the \MPIR library.}
\end{wpdeliv}
  \begin{wpdeliv}[due=12,id=MPIRfat,dissem=PU,nature=DEM,lead=UK]
      {Provide FAT binary support for all new x86\_64 processors, build system
        maintenance and improvements to tuning, profiling and testing subsystems
      for the \MPIR library.}
\end{wpdeliv}
  \begin{wpdeliv}[due=12,id=HPCcombi,dissem=PU,nature=DEM,lead=LL]
      {Explore the possibility to interface smoothly \Pythran, \Cython and \texttt{Cilk++}.}
\end{wpdeliv}

  \begin{wpdeliv}[due=12,id=LinBox-DSL,dissem=PU,nature=R,lead=UJF]
    {Library design and domain specific language exposing \Linbox parallel features to \Sage}
  \end{wpdeliv}
  \begin{wpdeliv}[due=18,id=GAP-hpc-report,dissem=PU,nature=R,lead=SA]
    {Report on development of designs for the \GAP developments --
      parallel library, interacts to standard infrastructure and
      \Cython-like extensions }
  \end{wpdeliv}
  \begin{wpdeliv}[due=24,id=MPIRprocessors,dissem=PU,nature=DEM,lead=UK]
      {Ongoing support of Intel, AMD, ARM, Sparc processors and new Operating System versions}
  \end{wpdeliv}
  \begin{wpdeliv}[due=24,id=HPCcombi,dissem=PU,nature=DEM,lead=LL]
      {Refactor the existing combinatorics \Sage code using the new developed \Pythran and \Cython features.}
  \end{wpdeliv}
  \begin{wpdeliv}[due=24,id=LinBox-algo,dissem=PU,nature=DEM, lead=UJF]
    {Exact linear algebra algorithms and implementations. Library maintenance and close integration
      in mathematical software for \Linbox library}
  \end{wpdeliv}
  \begin{wpdeliv}[due=48,id=LinBox-distributed,dissem=PU,nature=DEM, lead=UJF]
    {Implementations of exact linear algebra algorithms on distributed memory et heterogenous
      architectures: clusters and accelerators. Solving large linear systems
      over the rationals is the target application.} 
  \end{wpdeliv}
  \begin{wpdeliv}[due=47,id=GAP-software-final,dissem=PU,nature=OTHER,lead=SA]
      {Implementations of the \GAP developments, ready for release}
  \end{wpdeliv}
  \begin{wpdeliv}[due=48,id=GAP-APIs-report,dissem=PU,nature=R,lead=SA]
      {final report and evaluation of the \GAP developments}
  \end{wpdeliv}
\end{wpdelivs}
\end{workpackage}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../proposal"
%%% End:

%  LocalWords:  workpackage hpc wphases Pythran Linbox wpobjectives wpdescription texttt
%  LocalWords:  localtaskref Cython tasklist TOWRITE parallelisation Groebner bignum Cilk
%  LocalWords:  superoptimisation optimised Sparc hand-optimising superoptimising Numpy
%  LocalWords:  specialised optimising hpc-combi deployment-distrib delivref mpi4py
%  LocalWords:  optimisations wpdelivs wpdeliv dissem LinBox-algo Parallelise QS-linalg
%  LocalWords:  Block-Wiederman singular-polymul singular-polyarith MPIRsuperoptimiser
%  LocalWords:  superoptimiser MPIRprocessors MPIRfat HPCcombi compactitem taskref
%  LocalWords:  localdelivref optimisation Jupyter
